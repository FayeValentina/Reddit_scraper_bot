import logging
import asyncio
import sqlite3
from datetime import datetime, timedelta
from typing import Optional, Tuple, List

logger = logging.getLogger(__name__)

class AutoScraperManager:
    """è‡ªåŠ¨çˆ¬å–ç®¡ç†ç±»ï¼Œè´Ÿè´£Redditå†…å®¹çˆ¬å–å’Œå‘å¸ƒé€»è¾‘"""
    
    def __init__(self, reddit_scraper, ai_evaluator, twitter_manager, data_processor, config_manager, notification_callback=None):
        self.reddit_scraper = reddit_scraper
        self.ai_evaluator = ai_evaluator
        self.twitter_manager = twitter_manager
        self.data_processor = data_processor
        self.config_manager = config_manager
        self.notification_callback = notification_callback
        
        # è‡ªåŠ¨çˆ¬å–ä»»åŠ¡ç®¡ç†
        self.auto_scraper_task = None
        self.auto_scraper_running = False
        self.last_scrape_time = None
        self.next_scrape_time = None
    
    async def auto_scrape_and_post(self):
        """è‡ªåŠ¨çˆ¬å–Redditè¯„è®ºå¹¶å‘å¸ƒæœ€ä½³è¯„è®ºåˆ°Twitter"""
        try:
            # æ›´æ–°çˆ¬å–æ—¶é—´æˆ³
            self.last_scrape_time = datetime.now()
            logger.info(f"å¼€å§‹è‡ªåŠ¨çˆ¬å–... {self.last_scrape_time.strftime('%Y-%m-%d %H:%M:%S')}")
            
            # ä»é…ç½®ç®¡ç†å™¨è·å–å‚æ•°
            subreddits = self.config_manager.get_config('REDDIT_SUBREDDITS', ['python'])
            post_fetch_count = self.config_manager.get_config('REDDIT_POST_FETCH_COUNT', 50)
            sort_method = self.config_manager.get_config('REDDIT_SORT_METHOD', 'hot')
            time_filter = self.config_manager.get_config('REDDIT_TIME_FILTER', 'day')
            comments_per_post = self.config_manager.get_config('REDDIT_COMMENTS_PER_POST', 20)
            top_comments_count = self.config_manager.get_config('TOP_COMMENTS_COUNT', 50)
            gemini_batch_size = self.config_manager.get_config('GEMINI_BATCH_SIZE', 10)
            
            logger.info(f"çˆ¬å–é…ç½®: subreddits={subreddits}, posts={post_fetch_count}, sort={sort_method}")
            
            # çˆ¬å–Redditè¯„è®º
            all_comments, scrape_duration = await self._scrape_reddit_comments(
                subreddits, post_fetch_count, sort_method, time_filter, comments_per_post
            )
            
            if not all_comments:
                logger.warning("æœªè·å–åˆ°ä»»ä½•è¯„è®º")
                await self._send_notification("âš ï¸ è‡ªåŠ¨çˆ¬å–å¤±è´¥ï¼šæœªè·å–åˆ°ä»»ä½•è¯„è®º")
                return
            
            # AIè´¨é‡ç­›é€‰
            filtered_comments, api_calls = await self._filter_comments_with_ai(
                all_comments, top_comments_count, gemini_batch_size
            )
            
            if not filtered_comments:
                logger.warning("AIç­›é€‰åæ— é«˜è´¨é‡è¯„è®º")
                await self._send_notification("âš ï¸ AIç­›é€‰åæ— é«˜è´¨é‡è¯„è®ºå¯å‘å¸ƒ")
                return
            
            # é€‰æ‹©åˆé€‚çš„è¯„è®ºå‘å¸ƒ
            result, selected_comment = await self._select_and_post_comment(filtered_comments, api_calls, scrape_duration)
            
            if result == "all_duplicate":
                logger.warning("æœ¬æ¬¡çˆ¬å–çš„æ‰€æœ‰å†…å®¹éƒ½å·²ç»åœ¨Twitterå‘å¸ƒè¿‡")
                await self._send_notification("ğŸ“„ æœ¬æ¬¡çˆ¬å–çš„æ‰€æœ‰å†…å®¹éƒ½å·²ç»åœ¨Twitterå‘å¸ƒè¿‡ï¼")
            elif result:
                logger.info("è‡ªåŠ¨å‘å¸ƒæˆåŠŸ")
            else:
                logger.error("è‡ªåŠ¨å‘å¸ƒå¤±è´¥")
                
        except Exception as e:
            logger.error(f"è‡ªåŠ¨çˆ¬å–å’Œå‘å¸ƒæ—¶å‡ºé”™: {e}")
            await self._send_notification(f"âŒ è‡ªåŠ¨çˆ¬å–ç³»ç»Ÿå‡ºé”™: {str(e)}")
    
    async def _scrape_reddit_comments(self, subreddits, post_fetch_count, sort_method, time_filter, comments_per_post):
        """çˆ¬å–Redditè¯„è®º"""
        all_comments = []
        
        # å‡†å¤‡å¹¶å‘çˆ¬å–é…ç½®
        subreddit_configs = []
        for subreddit in subreddits:
            config = {
                'name': subreddit,
                'limit': post_fetch_count,
                'sort_by': sort_method,
                'comments_limit': comments_per_post,
                'time_filter': time_filter
            }
            subreddit_configs.append(config)
        
        logger.info(f"å¼€å§‹å¹¶å‘çˆ¬å– {len(subreddit_configs)} ä¸ªsubreddit...")
        scrape_start_time = datetime.now()
        
        try:
            scraped_data = await self.reddit_scraper.scrape_multiple_subreddits_concurrent(subreddit_configs)
            
            scrape_end_time = datetime.now()
            scrape_duration = (scrape_end_time - scrape_start_time).total_seconds()
            
            # ç»Ÿè®¡æ•°æ®
            total_posts = 0
            successful_subreddits = 0
            
            # åˆå¹¶æ‰€æœ‰è¯„è®ºæ•°æ®
            for subreddit_name, (posts_data, comments_data) in scraped_data.items():
                total_posts += len(posts_data) if posts_data else 0
                
                if comments_data:
                    logger.info(f"ä» r/{subreddit_name} è·å–äº† {len(posts_data)} ä¸ªå¸–å­ï¼Œ{len(comments_data)} æ¡è¯„è®º")
                    all_comments.extend(comments_data)
                    successful_subreddits += 1
                else:
                    logger.warning(f"ä» r/{subreddit_name} æœªè·å–åˆ°è¯„è®º")
            
            # æ€§èƒ½ç»Ÿè®¡
            comments_per_second = len(all_comments) / scrape_duration if scrape_duration > 0 else 0
            logger.info(
                f"ğŸš€ å¹¶å‘çˆ¬å–å®Œæˆï¼šç”¨æ—¶ {scrape_duration:.2f}ç§’ï¼Œ"
                f"æˆåŠŸçˆ¬å– {successful_subreddits}/{len(subreddit_configs)} ä¸ªsubredditï¼Œ"
                f"æ€»è®¡ {total_posts} ä¸ªå¸–å­ï¼Œ{len(all_comments)} æ¡è¯„è®ºï¼Œ"
                f"å¹³å‡é€Ÿåº¦ {comments_per_second:.1f} è¯„è®º/ç§’"
            )
            
            return all_comments, scrape_duration
                    
        except Exception as e:
            logger.error(f"å¹¶å‘çˆ¬å–æ—¶å‡ºé”™: {e}")
            await self._send_notification(f"âŒ å¹¶å‘çˆ¬å–å¤±è´¥: {str(e)}")
            return [], 0
    
    async def _filter_comments_with_ai(self, all_comments, top_comments_count, gemini_batch_size):
        """ä½¿ç”¨AIç­›é€‰è¯„è®ºè´¨é‡"""
        logger.info(f"æ€»å…±è·å–äº† {len(all_comments)} æ¡è¯„è®º")
        
        # æŒ‰åˆ†æ•°æ’åºï¼Œå–å‰Næ¡
        sorted_comments = sorted(all_comments, key=lambda x: x.get('score', 0), reverse=True)
        top_comments = sorted_comments[:top_comments_count]
        
        logger.info(f"é€‰æ‹©å‰ {len(top_comments)} æ¡é«˜åˆ†è¯„è®ºè¿›è¡ŒAIç­›é€‰")
        
        # AIè´¨é‡ç­›é€‰
        if self.ai_evaluator.is_available():
            filtered_comments, api_calls = await self.ai_evaluator.filter_comments_with_ai(top_comments, gemini_batch_size)
            logger.info(f"AIç­›é€‰å®Œæˆï¼Œä½¿ç”¨äº† {api_calls} æ¬¡APIè°ƒç”¨ï¼Œè·å¾— {len(filtered_comments)} æ¡é«˜è´¨é‡è¯„è®º")
            return filtered_comments, api_calls
        else:
            # å¦‚æœæ²¡æœ‰AIè¯„ä¼°ï¼Œç›´æ¥ä½¿ç”¨è¯„åˆ†æ’åºçš„ç»“æœ
            filtered_comments = top_comments[:10]
            for comment in filtered_comments:
                comment['confidence'] = 0.9
                comment['reason'] = 'åŸºäºè¯„åˆ†æ’åºï¼ˆæœªä½¿ç”¨AIç­›é€‰ï¼‰'
            logger.info("æœªé…ç½®AIè¯„ä¼°ï¼Œä½¿ç”¨è¯„åˆ†æ’åº")
            return filtered_comments, 0
    
    async def _select_and_post_comment(self, filtered_comments, api_call_count, scrape_duration=0):
        """æ™ºèƒ½é€‰æ‹©è¯„è®ºå¹¶å‘å¸ƒï¼Œé¿å…é‡å¤å†…å®¹"""
        try:
            # æŒ‰ç½®ä¿¡åº¦æ’åºè¯„è®º
            sorted_comments = sorted(filtered_comments, key=lambda x: x.get('confidence', 0), reverse=True)
            
            for i, comment in enumerate(sorted_comments):
                content = comment.get('body', '')
                if len(content) > 280:
                    content = content[:277] + "..."
                
                # æ£€æŸ¥æ˜¯å¦é‡å¤
                is_duplicate = await self._check_duplicate_content(content)
                
                if not is_duplicate:
                    # æ‰¾åˆ°éé‡å¤å†…å®¹ï¼Œç›´æ¥å‘å¸ƒ
                    logger.info(f"é€‰æ‹©ç¬¬{i+1}ä¼˜å…ˆè¯„è®ºå‘å¸ƒï¼Œç½®ä¿¡åº¦: {comment.get('confidence', 0):.2f}")
                    success = await self._auto_post_to_twitter(comment, api_call_count, scrape_duration)
                    return success, comment
                else:
                    logger.info(f"ç¬¬{i+1}ä¼˜å…ˆè¯„è®ºé‡å¤ï¼Œå°è¯•ä¸‹ä¸€ä¸ª...")
            
            # æ‰€æœ‰è¯„è®ºéƒ½é‡å¤ï¼Œè·³è¿‡å‘å¸ƒ
            if sorted_comments:
                logger.warning("æ‰€æœ‰é«˜è´¨é‡è¯„è®ºéƒ½é‡å¤ï¼Œè·³è¿‡æœ¬æ¬¡å‘å¸ƒ")
                return "all_duplicate", None
            
            return False, None
            
        except Exception as e:
            logger.error(f"é€‰æ‹©å’Œå‘å¸ƒè¯„è®ºæ—¶å‡ºé”™: {e}")
            return False, None
    
    async def _check_duplicate_content(self, content):
        """æ£€æŸ¥å†…å®¹æ˜¯å¦å·²ç»å‘å¸ƒè¿‡"""
        try:
            conn = sqlite3.connect('reddit_data.db')
            cursor = conn.cursor()
            
            # æŸ¥è¯¢æœ€è¿‘7å¤©å†…æ˜¯å¦æœ‰ç›¸åŒå†…å®¹
            seven_days_ago = datetime.now() - timedelta(days=7)
            cursor.execute("""
                SELECT COUNT(*) FROM reddit_comments 
                WHERE body = ? AND sent_at > ? AND tweet_id IS NOT NULL
            """, (content, seven_days_ago.strftime('%Y-%m-%d %H:%M:%S')))
            
            count = cursor.fetchone()[0]
            conn.close()
            
            return count > 0
            
        except Exception as e:
            logger.error(f"æ£€æŸ¥é‡å¤å†…å®¹æ—¶å‡ºé”™: {e}")
            return False
    
    async def _auto_post_to_twitter(self, comment, api_call_count, scrape_duration=0):
        """è‡ªåŠ¨å‘å¸ƒè¯„è®ºåˆ°Twitter"""
        try:
            content = comment.get('body', '')
            if len(content) > 280:
                content = content[:277] + "..."
            
            result = await self.twitter_manager.post_text_tweet(content)
            
            if result['success']:
                # æ›´æ–°è¯„è®ºæ•°æ®
                comment['tweet_id'] = result['tweet_id']
                comment['sent_at'] = datetime.now()
                comment['api_call_count'] = api_call_count
                comment['body'] = result['content']
                
                # ä¿å­˜åˆ°æ•°æ®åº“
                self.data_processor.save_comments_to_database([comment])
                
                # å‘é€æˆåŠŸé€šçŸ¥
                await self._send_auto_post_notification(comment, api_call_count, scrape_duration)
                
                return True
            else:
                # å‘å¸ƒå¤±è´¥æ—¶ï¼Œä¼ é€’å†…å®¹å’Œè¯„è®ºä¿¡æ¯
                await self._handle_twitter_error(result, content, comment)
                return False
                
        except Exception as e:
            logger.error(f"è‡ªåŠ¨å‘å¸ƒåˆ°Twitterå¤±è´¥: {e}")
            
            # æ ¼å¼åŒ–å†…å®¹ä¿¡æ¯ç”¨äºå¼‚å¸¸é€šçŸ¥
            display_content = content[:200] + "..." if len(content) > 200 else content
            content_info = f"\n\nğŸ“ <b>å‡†å¤‡å‘å¸ƒçš„å†…å®¹:</b>\n<code>{display_content}</code>"
            
            # æ ¼å¼åŒ–è¯„è®ºæ¥æºä¿¡æ¯
            source_info = ""
            if comment:
                source_info = f"\n\nğŸ”— <b>å†…å®¹æ¥æº:</b>\n"
                source_info += f"â€¢ æ¿å—: r/{comment.get('subreddit', 'unknown')}\n"
                source_info += f"â€¢ Redditè¯„åˆ†: {comment.get('score', 0)}\n"
                if comment.get('confidence'):
                    source_info += f"â€¢ AIç½®ä¿¡åº¦: {comment.get('confidence', 0):.2f}\n"
                if comment.get('reason'):
                    source_info += f"â€¢ AIè¯„ä»·: {comment.get('reason', 'æ— ')}"
            
            await self._send_notification(
                f"âŒ <b>å‘å¸ƒåˆ°Twitteræ—¶å‘ç”Ÿå¼‚å¸¸</b>\n\n"
                f"å¼‚å¸¸è¯¦æƒ…: {str(e)}\n\n"
                f"ğŸ’¡ è¿™å¯èƒ½æ˜¯ç³»ç»Ÿçº§é”™è¯¯ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥å’ŒAPIçŠ¶æ€ã€‚"
                f"{content_info}{source_info}"
            )
            return False
    
    async def _handle_twitter_error(self, result, content=None, comment_info=None):
        """å¤„ç†Twitter APIé”™è¯¯ï¼ŒåŒ…å«å‡†å¤‡å‘å¸ƒçš„å†…å®¹ä¿¡æ¯"""
        error_type = result.get('error_type', 'unknown')
        error_msg = result.get('error', 'Unknown error')
        
        # æ ¼å¼åŒ–å†…å®¹ä¿¡æ¯
        content_info = ""
        if content:
            display_content = content[:200] + "..." if len(content) > 200 else content
            content_info = f"\n\nğŸ“ <b>å‡†å¤‡å‘å¸ƒçš„å†…å®¹:</b>\n<code>{display_content}</code>"
        
        # æ ¼å¼åŒ–è¯„è®ºæ¥æºä¿¡æ¯
        source_info = ""
        if comment_info:
            source_info = f"\n\nğŸ”— <b>å†…å®¹æ¥æº:</b>\n"
            source_info += f"â€¢ æ¿å—: r/{comment_info.get('subreddit', 'unknown')}\n"
            source_info += f"â€¢ Redditè¯„åˆ†: {comment_info.get('score', 0)}\n"
            if comment_info.get('confidence'):
                source_info += f"â€¢ AIç½®ä¿¡åº¦: {comment_info.get('confidence', 0):.2f}\n"
            if comment_info.get('reason'):
                source_info += f"â€¢ AIè¯„ä»·: {comment_info.get('reason', 'æ— ')}"
        
        # æ ¹æ®é”™è¯¯ç±»å‹å‘é€ä¸åŒçš„é€šçŸ¥
        if error_type == 'permission':
            await self._send_notification(
                f"ğŸš« <b>Twitter APIæƒé™ä¸è¶³</b>\n\n"
                f"é”™è¯¯è¯¦æƒ…: {error_msg}\n\n"
                f"å¯èƒ½åŸå› :\n"
                f"â€¢ APIå¯†é’¥æƒé™ä¸è¶³ï¼ˆéœ€è¦Read and Writeæƒé™ï¼‰\n"
                f"â€¢ Twitterå¼€å‘è€…è´¦æˆ·è¢«é™åˆ¶\n"
                f"â€¢ APIè®¿é—®çº§åˆ«ä¸å¤Ÿï¼ˆéœ€è¦Basicæˆ–ä»¥ä¸Šï¼‰"
                f"{content_info}{source_info}"
            )
        elif error_type == 'authentication':
            await self._send_notification(
                f"ğŸ” <b>Twitter APIè®¤è¯å¤±è´¥</b>\n\n"
                f"é”™è¯¯è¯¦æƒ…: {error_msg}\n\n"
                f"è¯·æ£€æŸ¥APIå¯†é’¥æ˜¯å¦æ­£ç¡®é…ç½®"
                f"{content_info}{source_info}"
            )
        elif error_type == 'duplicate':
            await self._send_notification(
                f"âš ï¸ <b>Twitteræ£€æµ‹åˆ°é‡å¤å†…å®¹</b>\n\n"
                f"ç³»ç»Ÿçš„é‡å¤æ£€æµ‹å¯èƒ½å­˜åœ¨æ¼æ´ï¼ŒTwitterä»ç„¶æ£€æµ‹åˆ°é‡å¤ã€‚"
                f"{content_info}{source_info}"
            )
        elif error_type == 'forbidden':
            await self._send_notification(
                f"ğŸš« <b>Twitterå†…å®¹è¢«æ‹’ç»</b>\n\n"
                f"é”™è¯¯è¯¦æƒ…: {error_msg}\n\n"
                f"å¯èƒ½åŸå› :\n"
                f"â€¢ å†…å®¹è¿åTwitterç¤¾åŒºå‡†åˆ™\n"
                f"â€¢ åŒ…å«æ•æ„Ÿè¯æ±‡æˆ–é“¾æ¥\n"
                f"â€¢ å†…å®¹æ ¼å¼ä¸å½“\n"
                f"â€¢ è´¦æˆ·è¢«ä¸´æ—¶é™åˆ¶"
                f"{content_info}{source_info}"
            )
        elif error_type == 'file_too_large':
            await self._send_notification(
                f"ğŸ“ <b>æ–‡ä»¶è¿‡å¤§</b>\n\n"
                f"é”™è¯¯è¯¦æƒ…: {error_msg}\n\n"
                f"åª’ä½“æ–‡ä»¶è¶…è¿‡Twitteré™åˆ¶"
                f"{content_info}{source_info}"
            )
        else:
            await self._send_notification(
                f"âŒ <b>Twitterå‘å¸ƒå¤±è´¥</b>\n\n"
                f"é”™è¯¯ç±»å‹: {error_type}\n"
                f"é”™è¯¯è¯¦æƒ…: {error_msg}\n\n"
                f"ğŸ’¡ è¿™å¯èƒ½æ˜¯ä¸€ä¸ªæ–°çš„é”™è¯¯ç±»å‹ï¼Œè¯·æ£€æŸ¥å†…å®¹æ˜¯å¦æœ‰ç‰¹æ®Šå­—ç¬¦æˆ–æ ¼å¼é—®é¢˜ã€‚"
                f"{content_info}{source_info}"
            )
    
    async def _send_auto_post_notification(self, comment, api_call_count, scrape_duration=0):
        """å‘é€è‡ªåŠ¨å‘å¸ƒçš„é€šçŸ¥"""
        try:
            content = comment.get('body', '')
            display_content = content[:100] + "..." if len(content) > 100 else content
            
            performance_info = ""
            if scrape_duration > 0:
                performance_info = f"\nâš¡ <b>çˆ¬å–æ€§èƒ½:</b> ç”¨æ—¶ {scrape_duration:.2f}ç§’"
            
            notification = f"""
ğŸ¤– <b>è‡ªåŠ¨å‘å¸ƒæˆåŠŸ</b>

ğŸ“ <b>å‘å¸ƒå†…å®¹:</b> 
{display_content}

â° <b>å‘å¸ƒæ—¶é—´:</b> {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

ğŸ¤– <b>AIè¯„ä¼°:</b>
â€¢ ç½®ä¿¡åº¦: {comment.get('confidence', 0):.2f}
â€¢ è¯„ä»·: {comment.get('reason', 'æ— ')}

ğŸ“Š <b>èµ„æºä½¿ç”¨:</b>
â€¢ Gemini APIè°ƒç”¨: {api_call_count} æ¬¡{performance_info}

ğŸ”— <b>æ¥æº:</b> r/{comment.get('subreddit', 'unknown')}
â­ <b>Redditè¯„åˆ†:</b> {comment.get('score', 0)}
            """.strip()
            
            await self._send_notification(notification)
            
        except Exception as e:
            logger.error(f"å‘é€è‡ªåŠ¨å‘å¸ƒé€šçŸ¥å¤±è´¥: {e}")
    
    async def start_auto_scraper(self):
        """å¯åŠ¨è‡ªåŠ¨çˆ¬å–ä»»åŠ¡"""
        try:
            logger.info("è‡ªåŠ¨çˆ¬å–ä»»åŠ¡å·²å¯åŠ¨ï¼Œç­‰å¾…å¼€å…³å¯ç”¨...")
            
            while True:
                # æ£€æŸ¥è‡ªåŠ¨çˆ¬å–å¼€å…³
                scraper_enabled = self.config_manager.get_config('AUTO_SCRAPER_ENABLED', False)
                
                if scraper_enabled and not self.auto_scraper_running:
                    self.auto_scraper_running = True
                    fetch_interval = self.config_manager.get_config('REDDIT_FETCH_INTERVAL', 60)
                    logger.info(f"è‡ªåŠ¨çˆ¬å–å·²å¯ç”¨ï¼Œå°†åœ¨ {fetch_interval} åˆ†é’Ÿåå¼€å§‹é¦–æ¬¡çˆ¬å–")
                    await self._send_notification(f"ğŸ¤– è‡ªåŠ¨çˆ¬å–ç³»ç»Ÿå·²å¯åŠ¨ï¼Œå°†åœ¨ {fetch_interval} åˆ†é’Ÿåå¼€å§‹é¦–æ¬¡çˆ¬å–")
                    self.next_scrape_time = datetime.now() + timedelta(minutes=fetch_interval)
                elif not scraper_enabled and self.auto_scraper_running:
                    self.auto_scraper_running = False
                    self.next_scrape_time = None
                    logger.info("è‡ªåŠ¨çˆ¬å–å·²ç¦ç”¨")
                    await self._send_notification("â¸ï¸ è‡ªåŠ¨çˆ¬å–ç³»ç»Ÿå·²åœæ­¢")
                
                if scraper_enabled:
                    fetch_interval = self.config_manager.get_config('REDDIT_FETCH_INTERVAL', 60)
                    await asyncio.sleep(fetch_interval * 60)
                    await self.auto_scrape_and_post()
                    self.next_scrape_time = datetime.now() + timedelta(minutes=fetch_interval)
                else:
                    await asyncio.sleep(30)
                
        except asyncio.CancelledError:
            logger.info("è‡ªåŠ¨çˆ¬å–ä»»åŠ¡å·²åœæ­¢")
            self.auto_scraper_running = False
        except Exception as e:
            logger.error(f"è‡ªåŠ¨çˆ¬å–ä»»åŠ¡å‡ºé”™: {e}")
            self.auto_scraper_running = False
            await asyncio.sleep(300)
            await self.start_auto_scraper()
    
    def get_status_info(self) -> dict:
        """è·å–çˆ¬å–çŠ¶æ€ä¿¡æ¯"""
        scraper_enabled = self.config_manager.get_config('AUTO_SCRAPER_ENABLED', False)
        fetch_interval = self.config_manager.get_config('REDDIT_FETCH_INTERVAL', 60)
        
        status_info = {
            'enabled': scraper_enabled,
            'running': self.auto_scraper_running,
            'interval': fetch_interval,
            'last_scrape_time': self.last_scrape_time,
            'next_scrape_time': self.next_scrape_time
        }
        
        return status_info
    
    async def _send_notification(self, message: str):
        """å‘é€é€šçŸ¥æ¶ˆæ¯"""
        if self.notification_callback:
            await self.notification_callback(message)
    
    def update_next_scrape_time(self):
        """æ‰‹åŠ¨æ›´æ–°ä¸‹æ¬¡çˆ¬å–æ—¶é—´"""
        if self.auto_scraper_running:
            fetch_interval = self.config_manager.get_config('REDDIT_FETCH_INTERVAL', 60)
            self.next_scrape_time = datetime.now() + timedelta(minutes=fetch_interval)
    
    async def stop_auto_scraper(self):
        """åœæ­¢è‡ªåŠ¨çˆ¬å–ä»»åŠ¡"""
        if self.auto_scraper_task:
            self.auto_scraper_task.cancel()
            try:
                await self.auto_scraper_task
            except asyncio.CancelledError:
                pass
            self.auto_scraper_task = None
        
        self.auto_scraper_running = False
        self.next_scrape_time = None